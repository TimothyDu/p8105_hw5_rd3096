---
title: "p8105_hw5_rd3096"
author: Timothy Du
output: github_document
---

# load necessary pakages 
```{r setup}
library(tidyverse)
library(rvest)
library(ggplot2)
library(purrr)
library(patchwork)

set.seed(1)


knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```
## problem 1

we define a function to simulate birthdays and check for duplicates
```{r}
simulate_shared_birthday = function(n) {
  
  birthdays = sample(1:365, n, replace = TRUE)
  
  has_duplicate = any(duplicated(birthdays))
}

```
we run this 10000 timese based on a sample size of 2-50
```{r}
compute_duplicate_probability = function(n) {
  
  simulations = map(1:10000, ~simulate_shared_birthday(n)) %>% 
    unlist()
  
  prob = mean(simulations)
  
  # Return result as a tibble
  tibble(
    group_size = n,
    prob_duplicate = prob
  )
}

result_final = map_dfr(2:50, compute_duplicate_probability)

result_final
```

Make a plot showing the relationship between probability and group size
```{r}
ggplot(
  result_final, 
  aes(x = group_size, y = prob_duplicate)
  ) +
  geom_smooth() +
  geom_point(alpha=0.5) +
  labs(
    title = "Probability of Shared Birthday as a Function of Group Size",
    x = "Group Size",
    y = "Probability of Shared Birthday"
  ) +
  theme_minimal()
```

## problem 2

create one-sample t test function and power function

```{r}
simulation_test = function(mu, n=30,sigma=5) {
  
  x = rnorm(n=30, mean=mu, sd = sigma)

  one_sample_ttest = t.test (x,mu = 0) %>% 
    broom::tidy()
  
    tibble(
      mu_hat = one_sample_ttest$estimate,
      p_value = one_sample_ttest$p.value
    )
}

power_and_avemu = function(mu, n = 30, sigma = 5) {
 
  simulation_test_results = map_dfr(1:5000, ~ simulation_test(mu, n = 30,sigma = 5))
  
  power = mean(simulation_test_results$p_value < 0.05, na.rm = TRUE)
  
  avg_mu_hat = mean(simulation_test_results$mu_hat, na.rm = TRUE)
  
  rejected_samples = simulation_test_results %>%
  filter(p_value < 0.05)
  
  avg_mu_rejected = mean(rejected_samples$mu_hat, na.rm = TRUE)
  
  # Return a tibble with the true mu and calculated power
  tibble(
    true_mu = mu,
    power = power,
    avg_mu=avg_mu_hat,
    avgmu_reject=avg_mu_rejected
  )
}

mu_values = c(1, 2, 3, 4, 5, 6)

# Calculate power for each value of mu
power_results = map_dfr(mu_values, power_and_avemu)

power_results

ggplot(
  power_results, aes(x = true_mu, y = power,color=true_mu)) +
  geom_line() +
  scale_color_viridis_c()+
  geom_point(alpha=0.8) +
  labs(
    title = "Power of One-Sample t-Test as a Function of Effect Size",
    x = "True Mean (Effect Size)",
    y = "Power"
  ) +
  theme_minimal()
```

Complete the two plots analyzing average estimate of mu_hat and true mu
```{r}
estimate_results = map_dfr(mu_values, power_and_avemu)

estimate_results 

mu_plot_1=
  ggplot(estimate_results, aes(x = true_mu, y = avg_mu, color=true_mu)) +
  geom_line() + 
  scale_color_viridis_c()+
  geom_point(size = 2) +     
  labs(
    title = "Average Estimate of μ̂ as a Function of True μ",
    x = "True Mean (μ)",
    y = "Average Estimate of μ̂"
  ) +
  theme_minimal()

mu_plot_2 =
  ggplot(estimate_results, aes(x = true_mu, y = avgmu_reject, color=true_mu)) +
  geom_line() + 
  scale_color_viridis_c()+
  geom_point(size = 2) +     
  labs(
    title = "Average Estimate of μ̂ Only in Rejected Samples",
    x = "True Mean (μ)",
    y = "Average Estimate of μ̂ (Rejected Samples)"
  ) +
  theme_minimal()
  
mu_plot_1+mu_plot_2
```

Right Plot (Rejected Samples Only): 

The sample average of mu across tests for which the null hypothesis was rejected is not approximately equal to the true value of mu.

This plot shows the average mu only for the samples in which the null hypothesis was rejected (i.e., samples with p-values < 0.05). In this case, the average estimate of mu is systematically higher than the true mu, particularly for lower values of mu, this suggests an upward bias due to only including samples with significant results.

This discrepancy occurs because we are only considering samples where the null hypothesis is rejected, which means we are selecting for samples where mu deviates sufficiently from 0 to yield a significant result. This introduces selection bias: when filtering for significant results, the sample means tend to be more extreme (in this case, higher than the true mu.

## problem3

we first read the homecide-data.csv file and describe this raw datafile

```{r}
homicide = read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/refs/heads/master/homicide-data.csv")

str(homicide)
```

The raw data consists of 52,179 rows and 12 columns. Each row represents a unique homicide case, this dataset provides detailed information about each homicide case, including geographic data, victim demographics, and case dispositions. Some typical columns include unid, reported_date, victim_last and vitcim_first name. The more detailed information about these columns are listed as follow:
uid: A unique identifier for each case (character).
reported_date: The date the homicide was reported, in YYYYMMDD format (numeric).
victim_last: The last name of the victim (character).
victim_first: The first name of the victim (character).
victim_race: The race of the victim (character).
victim_age: The age of the victim (character; possibly contains missing or non-numeric values).
victim_sex: The sex of the victim (character, likely "Male" or "Female").
city: The city where the homicide occurred (character).
state: The state abbreviation for where the homicide occurred (character).
lat: The latitude of the homicide location (numeric).
lon: The longitude of the homicide location (numeric).
disposition: The outcome or current status of the case, e.g., "Closed without arrest," "Closed by arrest," "Open/No arrest" (character).

Next step we will create a new variable and summarize within cities to get our desired total number of homicides and the number of unsolved homicides.

```{r}
homicide_summary = 
  homicide %>% 
  mutate(
    city_state = str_c(city, ",", state)
  ) %>% 
  group_by(city_state) %>% 
  summarize(
    total_homides = n(),
    total_unsolved= sum(disposition %in% c("Closed without arrest","Open/No arrest"))
  )

homicide_summary
```

